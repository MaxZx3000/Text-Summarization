{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Summarization Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import textwrap\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"bbc_text_cls.csv\"\n",
    "news_df = pd.read_csv(dataset_path, delimiter = ',')\n",
    "sample_news = news_df[news_df.labels == \"business\"]['text'].sample(random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Any Unnecessary Unicode Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "480    Christmas sales worst since 1981\\n\\nUK retail ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_sample_news = sample_news.replace('\\n', \"\")\\\n",
    "                               .replace('\\t', \"\")\\\n",
    "                               .replace('=', \"\")\n",
    "                             \n",
    "stripped_sample_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Christmas sales worst since 1981\\n\\nUK retail sales fell in December, failing to meet expectations and making it by some counts the worst Christmas since 1981.',\n",
       " 'Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said.',\n",
       " 'The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%.',\n",
       " 'A number of retailers have already reported poor figures for December.',\n",
       " 'Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.',\n",
       " 'The last time retailers endured a tougher Christmas was 23 years previously, when sales plunged 1.7%.',\n",
       " 'The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures.',\n",
       " 'Some analysts put a positive gloss on the figures, pointing out that the non-seasonally-adjusted figures showed a performance comparable with 2003.',\n",
       " 'The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s.',\n",
       " 'And figures for retail volume outperformed measures of actual spending, an indication that consumers are looking for bargains, and retailers are cutting their prices.',\n",
       " 'However, reports from some High Street retailers highlight the weakness of the sector.',\n",
       " 'Morrisons, Woolworths, House of Fraser, Marks & Spencer and Big Food all said that the festive period was disappointing.',\n",
       " 'And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years.',\n",
       " 'Yet, other retailers - including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that festive sales were well up on last year.',\n",
       " 'Investec chief economist Philip Shaw said he did not expect the poor retail figures to have any immediate effect on interest rates.',\n",
       " '\"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don\\'t really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw.',\n",
       " '\"Our view is the Bank of England will keep its powder dry and wait to see the big picture.\"']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokenized = sent_tokenize(sample_news.iloc[0])\n",
    "sentence_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<17x199 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 319 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def perform_tokenizer(sentence):\n",
    "#     return sent_tokenize(sentence)\n",
    "\n",
    "tf_idf_vectorizer = TfidfVectorizer(norm = 'l1')\n",
    "tf_idf_matrix = tf_idf_vectorizer.fit_transform(sentence_tokenized)\n",
    "tf_idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us M * M matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_matrix = cosine_similarity(\n",
    "    tf_idf_matrix\n",
    ")\n",
    "cosine_similarity_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.194654</td>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.034468</td>\n",
       "      <td>0.106553</td>\n",
       "      <td>0.124564</td>\n",
       "      <td>0.053690</td>\n",
       "      <td>0.037925</td>\n",
       "      <td>0.087921</td>\n",
       "      <td>0.043813</td>\n",
       "      <td>0.050636</td>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.192669</td>\n",
       "      <td>0.057304</td>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.091390</td>\n",
       "      <td>0.050011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.194654</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.240532</td>\n",
       "      <td>0.100899</td>\n",
       "      <td>0.068917</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.099728</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>0.182963</td>\n",
       "      <td>0.080670</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.107054</td>\n",
       "      <td>0.081372</td>\n",
       "      <td>0.038076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.078035</td>\n",
       "      <td>0.240532</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>0.185687</td>\n",
       "      <td>0.039309</td>\n",
       "      <td>0.201025</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.172610</td>\n",
       "      <td>0.018840</td>\n",
       "      <td>0.172072</td>\n",
       "      <td>0.057154</td>\n",
       "      <td>0.114068</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.118324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034468</td>\n",
       "      <td>0.100899</td>\n",
       "      <td>0.034798</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.073740</td>\n",
       "      <td>0.046836</td>\n",
       "      <td>0.173604</td>\n",
       "      <td>0.073493</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>0.180949</td>\n",
       "      <td>0.086262</td>\n",
       "      <td>0.030669</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.112473</td>\n",
       "      <td>0.171821</td>\n",
       "      <td>0.073067</td>\n",
       "      <td>0.029816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.106553</td>\n",
       "      <td>0.068917</td>\n",
       "      <td>0.185687</td>\n",
       "      <td>0.073740</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>0.116128</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.074554</td>\n",
       "      <td>0.103756</td>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.094375</td>\n",
       "      <td>0.110076</td>\n",
       "      <td>0.084111</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>0.079990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.124564</td>\n",
       "      <td>0.058394</td>\n",
       "      <td>0.039309</td>\n",
       "      <td>0.046836</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.025358</td>\n",
       "      <td>0.065901</td>\n",
       "      <td>0.054528</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>0.096114</td>\n",
       "      <td>0.022454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.053690</td>\n",
       "      <td>0.099728</td>\n",
       "      <td>0.201025</td>\n",
       "      <td>0.173604</td>\n",
       "      <td>0.088796</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079867</td>\n",
       "      <td>0.075787</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.119405</td>\n",
       "      <td>0.037846</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.146908</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.161015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.037925</td>\n",
       "      <td>0.068686</td>\n",
       "      <td>0.061682</td>\n",
       "      <td>0.073493</td>\n",
       "      <td>0.116128</td>\n",
       "      <td>0.020982</td>\n",
       "      <td>0.079867</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.158891</td>\n",
       "      <td>0.062966</td>\n",
       "      <td>0.084862</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.054856</td>\n",
       "      <td>0.056557</td>\n",
       "      <td>0.090770</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.035234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.087921</td>\n",
       "      <td>0.182963</td>\n",
       "      <td>0.172610</td>\n",
       "      <td>0.041766</td>\n",
       "      <td>0.084500</td>\n",
       "      <td>0.106910</td>\n",
       "      <td>0.075787</td>\n",
       "      <td>0.158891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.104252</td>\n",
       "      <td>0.059426</td>\n",
       "      <td>0.069854</td>\n",
       "      <td>0.074659</td>\n",
       "      <td>0.023871</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.051570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.043813</td>\n",
       "      <td>0.080670</td>\n",
       "      <td>0.018840</td>\n",
       "      <td>0.180949</td>\n",
       "      <td>0.074554</td>\n",
       "      <td>0.025358</td>\n",
       "      <td>0.070096</td>\n",
       "      <td>0.062966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046704</td>\n",
       "      <td>0.080105</td>\n",
       "      <td>0.171232</td>\n",
       "      <td>0.077569</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>0.146167</td>\n",
       "      <td>0.053077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050636</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>0.172072</td>\n",
       "      <td>0.086262</td>\n",
       "      <td>0.103756</td>\n",
       "      <td>0.065901</td>\n",
       "      <td>0.119405</td>\n",
       "      <td>0.084862</td>\n",
       "      <td>0.104252</td>\n",
       "      <td>0.046704</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.029719</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>0.071161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.024412</td>\n",
       "      <td>0.056024</td>\n",
       "      <td>0.057154</td>\n",
       "      <td>0.030669</td>\n",
       "      <td>0.039509</td>\n",
       "      <td>0.054528</td>\n",
       "      <td>0.037846</td>\n",
       "      <td>0.046667</td>\n",
       "      <td>0.059426</td>\n",
       "      <td>0.080105</td>\n",
       "      <td>0.049002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.119793</td>\n",
       "      <td>0.100299</td>\n",
       "      <td>0.042383</td>\n",
       "      <td>0.063169</td>\n",
       "      <td>0.116082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.192669</td>\n",
       "      <td>0.096984</td>\n",
       "      <td>0.114068</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.094375</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>0.021965</td>\n",
       "      <td>0.054856</td>\n",
       "      <td>0.069854</td>\n",
       "      <td>0.171232</td>\n",
       "      <td>0.028440</td>\n",
       "      <td>0.119793</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.058529</td>\n",
       "      <td>0.039089</td>\n",
       "      <td>0.062499</td>\n",
       "      <td>0.049533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.057304</td>\n",
       "      <td>0.061119</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.112473</td>\n",
       "      <td>0.110076</td>\n",
       "      <td>0.100456</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056557</td>\n",
       "      <td>0.074659</td>\n",
       "      <td>0.077569</td>\n",
       "      <td>0.029719</td>\n",
       "      <td>0.100299</td>\n",
       "      <td>0.058529</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>0.020661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.042087</td>\n",
       "      <td>0.107054</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.171821</td>\n",
       "      <td>0.084111</td>\n",
       "      <td>0.010393</td>\n",
       "      <td>0.146908</td>\n",
       "      <td>0.090770</td>\n",
       "      <td>0.023871</td>\n",
       "      <td>0.039420</td>\n",
       "      <td>0.021775</td>\n",
       "      <td>0.042383</td>\n",
       "      <td>0.039089</td>\n",
       "      <td>0.030344</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.101206</td>\n",
       "      <td>0.041055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.091390</td>\n",
       "      <td>0.081372</td>\n",
       "      <td>0.057687</td>\n",
       "      <td>0.073067</td>\n",
       "      <td>0.013052</td>\n",
       "      <td>0.096114</td>\n",
       "      <td>0.261500</td>\n",
       "      <td>0.048192</td>\n",
       "      <td>0.042659</td>\n",
       "      <td>0.146167</td>\n",
       "      <td>0.052985</td>\n",
       "      <td>0.063169</td>\n",
       "      <td>0.062499</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>0.101206</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.104910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.050011</td>\n",
       "      <td>0.038076</td>\n",
       "      <td>0.118324</td>\n",
       "      <td>0.029816</td>\n",
       "      <td>0.079990</td>\n",
       "      <td>0.022454</td>\n",
       "      <td>0.161015</td>\n",
       "      <td>0.035234</td>\n",
       "      <td>0.051570</td>\n",
       "      <td>0.053077</td>\n",
       "      <td>0.071161</td>\n",
       "      <td>0.116082</td>\n",
       "      <td>0.049533</td>\n",
       "      <td>0.020661</td>\n",
       "      <td>0.041055</td>\n",
       "      <td>0.104910</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.194654  0.078035  0.034468  0.106553  0.124564  0.053690   \n",
       "1   0.194654  1.000000  0.240532  0.100899  0.068917  0.058394  0.099728   \n",
       "2   0.078035  0.240532  1.000000  0.034798  0.185687  0.039309  0.201025   \n",
       "3   0.034468  0.100899  0.034798  1.000000  0.073740  0.046836  0.173604   \n",
       "4   0.106553  0.068917  0.185687  0.073740  1.000000  0.077995  0.088796   \n",
       "5   0.124564  0.058394  0.039309  0.046836  0.077995  1.000000  0.021637   \n",
       "6   0.053690  0.099728  0.201025  0.173604  0.088796  0.021637  1.000000   \n",
       "7   0.037925  0.068686  0.061682  0.073493  0.116128  0.020982  0.079867   \n",
       "8   0.087921  0.182963  0.172610  0.041766  0.084500  0.106910  0.075787   \n",
       "9   0.043813  0.080670  0.018840  0.180949  0.074554  0.025358  0.070096   \n",
       "10  0.050636  0.047505  0.172072  0.086262  0.103756  0.065901  0.119405   \n",
       "11  0.024412  0.056024  0.057154  0.030669  0.039509  0.054528  0.037846   \n",
       "12  0.192669  0.096984  0.114068  0.065425  0.094375  0.190303  0.021965   \n",
       "13  0.057304  0.061119  0.000000  0.112473  0.110076  0.100456  0.000000   \n",
       "14  0.042087  0.107054  0.058100  0.171821  0.084111  0.010393  0.146908   \n",
       "15  0.091390  0.081372  0.057687  0.073067  0.013052  0.096114  0.261500   \n",
       "16  0.050011  0.038076  0.118324  0.029816  0.079990  0.022454  0.161015   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.037925  0.087921  0.043813  0.050636  0.024412  0.192669  0.057304   \n",
       "1   0.068686  0.182963  0.080670  0.047505  0.056024  0.096984  0.061119   \n",
       "2   0.061682  0.172610  0.018840  0.172072  0.057154  0.114068  0.000000   \n",
       "3   0.073493  0.041766  0.180949  0.086262  0.030669  0.065425  0.112473   \n",
       "4   0.116128  0.084500  0.074554  0.103756  0.039509  0.094375  0.110076   \n",
       "5   0.020982  0.106910  0.025358  0.065901  0.054528  0.190303  0.100456   \n",
       "6   0.079867  0.075787  0.070096  0.119405  0.037846  0.021965  0.000000   \n",
       "7   1.000000  0.158891  0.062966  0.084862  0.046667  0.054856  0.056557   \n",
       "8   0.158891  1.000000  0.000000  0.104252  0.059426  0.069854  0.074659   \n",
       "9   0.062966  0.000000  1.000000  0.046704  0.080105  0.171232  0.077569   \n",
       "10  0.084862  0.104252  0.046704  1.000000  0.049002  0.028440  0.029719   \n",
       "11  0.046667  0.059426  0.080105  0.049002  1.000000  0.119793  0.100299   \n",
       "12  0.054856  0.069854  0.171232  0.028440  0.119793  1.000000  0.058529   \n",
       "13  0.056557  0.074659  0.077569  0.029719  0.100299  0.058529  1.000000   \n",
       "14  0.090770  0.023871  0.039420  0.021775  0.042383  0.039089  0.030344   \n",
       "15  0.048192  0.042659  0.146167  0.052985  0.063169  0.062499  0.044690   \n",
       "16  0.035234  0.051570  0.053077  0.071161  0.116082  0.049533  0.020661   \n",
       "\n",
       "          14        15        16  \n",
       "0   0.042087  0.091390  0.050011  \n",
       "1   0.107054  0.081372  0.038076  \n",
       "2   0.058100  0.057687  0.118324  \n",
       "3   0.171821  0.073067  0.029816  \n",
       "4   0.084111  0.013052  0.079990  \n",
       "5   0.010393  0.096114  0.022454  \n",
       "6   0.146908  0.261500  0.161015  \n",
       "7   0.090770  0.048192  0.035234  \n",
       "8   0.023871  0.042659  0.051570  \n",
       "9   0.039420  0.146167  0.053077  \n",
       "10  0.021775  0.052985  0.071161  \n",
       "11  0.042383  0.063169  0.116082  \n",
       "12  0.039089  0.062499  0.049533  \n",
       "13  0.030344  0.044690  0.020661  \n",
       "14  1.000000  0.101206  0.041055  \n",
       "15  0.101206  1.000000  0.104910  \n",
       "16  0.041055  0.104910  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    cosine_similarity_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide each row, so each row sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.27013241 2.58357841 2.60992313 2.33008611 2.40173849 2.06213552\n",
      " 2.61287022 2.09775769 2.33763944 2.17152224 2.13443788 1.97706875\n",
      " 2.4296146  1.93445416 2.05038987 2.34066007 2.04296773]\n"
     ]
    }
   ],
   "source": [
    "sum_all_rows = cosine_similarity_matrix.sum(axis = 1, dtype = 'float64')\n",
    "print(sum_all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17, 17)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "normalized_cosine_similarity_matrix = cosine_similarity_matrix / sum_all_rows.reshape(-1, 1)\n",
    "print(normalized_cosine_similarity_matrix.shape)\n",
    "print(normalized_cosine_similarity_matrix.sum(axis = 1))\n",
    "print(normalized_cosine_similarity_matrix[0].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Uniform Matrix\n",
    "\n",
    "It contains 1 / M. M represents number of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_sentences = normalized_cosine_similarity_matrix.shape[0]\n",
    "uniform_matrix = np.full(fill_value = 1 / number_of_sentences, shape = (number_of_sentences, number_of_sentences))\n",
    "\n",
    "uniform_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find A, by this formula:\n",
    "\n",
    "${A = αU + (1 - α) * G}$\n",
    "\n",
    "The formula is used for perform smoothing.\n",
    "\n",
    "Where:\n",
    "* G = Similarity Matrix\n",
    "* U = Uniform matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.383251</td>\n",
       "      <td>0.081707</td>\n",
       "      <td>0.038042</td>\n",
       "      <td>0.021729</td>\n",
       "      <td>0.048720</td>\n",
       "      <td>0.055464</td>\n",
       "      <td>0.028927</td>\n",
       "      <td>0.023024</td>\n",
       "      <td>0.041743</td>\n",
       "      <td>0.025228</td>\n",
       "      <td>0.027783</td>\n",
       "      <td>0.017964</td>\n",
       "      <td>0.080964</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.024582</td>\n",
       "      <td>0.043043</td>\n",
       "      <td>0.027549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.072865</td>\n",
       "      <td>0.337825</td>\n",
       "      <td>0.087959</td>\n",
       "      <td>0.042019</td>\n",
       "      <td>0.031497</td>\n",
       "      <td>0.028035</td>\n",
       "      <td>0.041634</td>\n",
       "      <td>0.031421</td>\n",
       "      <td>0.069019</td>\n",
       "      <td>0.035364</td>\n",
       "      <td>0.024453</td>\n",
       "      <td>0.027255</td>\n",
       "      <td>0.040732</td>\n",
       "      <td>0.028932</td>\n",
       "      <td>0.044045</td>\n",
       "      <td>0.035595</td>\n",
       "      <td>0.021350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034238</td>\n",
       "      <td>0.087160</td>\n",
       "      <td>0.334504</td>\n",
       "      <td>0.020156</td>\n",
       "      <td>0.069298</td>\n",
       "      <td>0.021626</td>\n",
       "      <td>0.074293</td>\n",
       "      <td>0.028912</td>\n",
       "      <td>0.065039</td>\n",
       "      <td>0.014959</td>\n",
       "      <td>0.064864</td>\n",
       "      <td>0.027438</td>\n",
       "      <td>0.045973</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.027746</td>\n",
       "      <td>0.027611</td>\n",
       "      <td>0.047359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.021397</td>\n",
       "      <td>0.045631</td>\n",
       "      <td>0.021518</td>\n",
       "      <td>0.373617</td>\n",
       "      <td>0.035723</td>\n",
       "      <td>0.025909</td>\n",
       "      <td>0.072153</td>\n",
       "      <td>0.035633</td>\n",
       "      <td>0.024060</td>\n",
       "      <td>0.074833</td>\n",
       "      <td>0.040291</td>\n",
       "      <td>0.020011</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>0.049853</td>\n",
       "      <td>0.071503</td>\n",
       "      <td>0.035478</td>\n",
       "      <td>0.019700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.046534</td>\n",
       "      <td>0.033214</td>\n",
       "      <td>0.074540</td>\n",
       "      <td>0.034921</td>\n",
       "      <td>0.362734</td>\n",
       "      <td>0.036427</td>\n",
       "      <td>0.040249</td>\n",
       "      <td>0.049922</td>\n",
       "      <td>0.038729</td>\n",
       "      <td>0.035209</td>\n",
       "      <td>0.045544</td>\n",
       "      <td>0.022806</td>\n",
       "      <td>0.042224</td>\n",
       "      <td>0.047781</td>\n",
       "      <td>0.038591</td>\n",
       "      <td>0.013443</td>\n",
       "      <td>0.037133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.060168</td>\n",
       "      <td>0.032893</td>\n",
       "      <td>0.025026</td>\n",
       "      <td>0.028129</td>\n",
       "      <td>0.040973</td>\n",
       "      <td>0.421018</td>\n",
       "      <td>0.017742</td>\n",
       "      <td>0.017472</td>\n",
       "      <td>0.052891</td>\n",
       "      <td>0.019276</td>\n",
       "      <td>0.035988</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>0.087265</td>\n",
       "      <td>0.050231</td>\n",
       "      <td>0.013108</td>\n",
       "      <td>0.048441</td>\n",
       "      <td>0.018079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.026290</td>\n",
       "      <td>0.041266</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.065299</td>\n",
       "      <td>0.037710</td>\n",
       "      <td>0.015862</td>\n",
       "      <td>0.334136</td>\n",
       "      <td>0.034805</td>\n",
       "      <td>0.033478</td>\n",
       "      <td>0.031627</td>\n",
       "      <td>0.047668</td>\n",
       "      <td>0.021135</td>\n",
       "      <td>0.015969</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.056614</td>\n",
       "      <td>0.093893</td>\n",
       "      <td>0.061204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.024191</td>\n",
       "      <td>0.036655</td>\n",
       "      <td>0.033817</td>\n",
       "      <td>0.038603</td>\n",
       "      <td>0.055878</td>\n",
       "      <td>0.017325</td>\n",
       "      <td>0.041185</td>\n",
       "      <td>0.414018</td>\n",
       "      <td>0.073205</td>\n",
       "      <td>0.034337</td>\n",
       "      <td>0.043209</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>0.031051</td>\n",
       "      <td>0.031740</td>\n",
       "      <td>0.045603</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.023100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.040793</td>\n",
       "      <td>0.075352</td>\n",
       "      <td>0.071587</td>\n",
       "      <td>0.024010</td>\n",
       "      <td>0.039549</td>\n",
       "      <td>0.047698</td>\n",
       "      <td>0.036381</td>\n",
       "      <td>0.066599</td>\n",
       "      <td>0.372438</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.046731</td>\n",
       "      <td>0.030432</td>\n",
       "      <td>0.034224</td>\n",
       "      <td>0.035970</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>0.024335</td>\n",
       "      <td>0.027575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.025973</td>\n",
       "      <td>0.040400</td>\n",
       "      <td>0.016198</td>\n",
       "      <td>0.079652</td>\n",
       "      <td>0.038006</td>\n",
       "      <td>0.018749</td>\n",
       "      <td>0.036261</td>\n",
       "      <td>0.033470</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.400254</td>\n",
       "      <td>0.027105</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.075849</td>\n",
       "      <td>0.039186</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.066038</td>\n",
       "      <td>0.029600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.028988</td>\n",
       "      <td>0.027742</td>\n",
       "      <td>0.077348</td>\n",
       "      <td>0.043176</td>\n",
       "      <td>0.050142</td>\n",
       "      <td>0.035067</td>\n",
       "      <td>0.056374</td>\n",
       "      <td>0.042618</td>\n",
       "      <td>0.050340</td>\n",
       "      <td>0.027423</td>\n",
       "      <td>0.407055</td>\n",
       "      <td>0.028338</td>\n",
       "      <td>0.020149</td>\n",
       "      <td>0.020659</td>\n",
       "      <td>0.017495</td>\n",
       "      <td>0.029924</td>\n",
       "      <td>0.037162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.019319</td>\n",
       "      <td>0.032910</td>\n",
       "      <td>0.033396</td>\n",
       "      <td>0.022009</td>\n",
       "      <td>0.025810</td>\n",
       "      <td>0.032267</td>\n",
       "      <td>0.025095</td>\n",
       "      <td>0.028887</td>\n",
       "      <td>0.034373</td>\n",
       "      <td>0.043263</td>\n",
       "      <td>0.029891</td>\n",
       "      <td>0.438753</td>\n",
       "      <td>0.060326</td>\n",
       "      <td>0.051945</td>\n",
       "      <td>0.027045</td>\n",
       "      <td>0.035982</td>\n",
       "      <td>0.058730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.076229</td>\n",
       "      <td>0.042754</td>\n",
       "      <td>0.048730</td>\n",
       "      <td>0.031712</td>\n",
       "      <td>0.041841</td>\n",
       "      <td>0.075401</td>\n",
       "      <td>0.016508</td>\n",
       "      <td>0.028015</td>\n",
       "      <td>0.033262</td>\n",
       "      <td>0.068729</td>\n",
       "      <td>0.018773</td>\n",
       "      <td>0.050733</td>\n",
       "      <td>0.358673</td>\n",
       "      <td>0.029300</td>\n",
       "      <td>0.022499</td>\n",
       "      <td>0.030689</td>\n",
       "      <td>0.026152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.034003</td>\n",
       "      <td>0.035679</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.058244</td>\n",
       "      <td>0.057191</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>0.008824</td>\n",
       "      <td>0.033675</td>\n",
       "      <td>0.041629</td>\n",
       "      <td>0.042907</td>\n",
       "      <td>0.021882</td>\n",
       "      <td>0.052895</td>\n",
       "      <td>0.034541</td>\n",
       "      <td>0.448224</td>\n",
       "      <td>0.022157</td>\n",
       "      <td>0.028460</td>\n",
       "      <td>0.017902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.026271</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.032909</td>\n",
       "      <td>0.080053</td>\n",
       "      <td>0.043692</td>\n",
       "      <td>0.013132</td>\n",
       "      <td>0.069725</td>\n",
       "      <td>0.046453</td>\n",
       "      <td>0.018719</td>\n",
       "      <td>0.025165</td>\n",
       "      <td>0.017850</td>\n",
       "      <td>0.026394</td>\n",
       "      <td>0.025028</td>\n",
       "      <td>0.021403</td>\n",
       "      <td>0.423379</td>\n",
       "      <td>0.050779</td>\n",
       "      <td>0.025843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.042011</td>\n",
       "      <td>0.038373</td>\n",
       "      <td>0.029772</td>\n",
       "      <td>0.035358</td>\n",
       "      <td>0.013563</td>\n",
       "      <td>0.043727</td>\n",
       "      <td>0.103786</td>\n",
       "      <td>0.026324</td>\n",
       "      <td>0.024315</td>\n",
       "      <td>0.061903</td>\n",
       "      <td>0.028065</td>\n",
       "      <td>0.031763</td>\n",
       "      <td>0.031520</td>\n",
       "      <td>0.025052</td>\n",
       "      <td>0.045576</td>\n",
       "      <td>0.371969</td>\n",
       "      <td>0.046921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.029631</td>\n",
       "      <td>0.024665</td>\n",
       "      <td>0.058054</td>\n",
       "      <td>0.021229</td>\n",
       "      <td>0.042104</td>\n",
       "      <td>0.018166</td>\n",
       "      <td>0.075816</td>\n",
       "      <td>0.023483</td>\n",
       "      <td>0.030280</td>\n",
       "      <td>0.030907</td>\n",
       "      <td>0.038431</td>\n",
       "      <td>0.057121</td>\n",
       "      <td>0.029432</td>\n",
       "      <td>0.017420</td>\n",
       "      <td>0.025905</td>\n",
       "      <td>0.052472</td>\n",
       "      <td>0.424885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.383251  0.081707  0.038042  0.021729  0.048720  0.055464  0.028927   \n",
       "1   0.072865  0.337825  0.087959  0.042019  0.031497  0.028035  0.041634   \n",
       "2   0.034238  0.087160  0.334504  0.020156  0.069298  0.021626  0.074293   \n",
       "3   0.021397  0.045631  0.021518  0.373617  0.035723  0.025909  0.072153   \n",
       "4   0.046534  0.033214  0.074540  0.034921  0.362734  0.036427  0.040249   \n",
       "5   0.060168  0.032893  0.025026  0.028129  0.040973  0.421018  0.017742   \n",
       "6   0.026290  0.041266  0.074219  0.065299  0.037710  0.015862  0.334136   \n",
       "7   0.024191  0.036655  0.033817  0.038603  0.055878  0.017325  0.041185   \n",
       "8   0.040793  0.075352  0.071587  0.024010  0.039549  0.047698  0.036381   \n",
       "9   0.025973  0.040400  0.016198  0.079652  0.038006  0.018749  0.036261   \n",
       "10  0.028988  0.027742  0.077348  0.043176  0.050142  0.035067  0.056374   \n",
       "11  0.019319  0.032910  0.033396  0.022009  0.025810  0.032267  0.025095   \n",
       "12  0.076229  0.042754  0.048730  0.031712  0.041841  0.075401  0.016508   \n",
       "13  0.034003  0.035679  0.008824  0.058244  0.057191  0.052964  0.008824   \n",
       "14  0.026271  0.053204  0.032909  0.080053  0.043692  0.013132  0.069725   \n",
       "15  0.042011  0.038373  0.029772  0.035358  0.013563  0.043727  0.103786   \n",
       "16  0.029631  0.024665  0.058054  0.021229  0.042104  0.018166  0.075816   \n",
       "\n",
       "          7         8         9         10        11        12        13  \\\n",
       "0   0.023024  0.041743  0.025228  0.027783  0.017964  0.080964  0.030280   \n",
       "1   0.031421  0.069019  0.035364  0.024453  0.027255  0.040732  0.028932   \n",
       "2   0.028912  0.065039  0.014959  0.064864  0.027438  0.045973  0.008824   \n",
       "3   0.035633  0.024060  0.074833  0.040291  0.020011  0.032690  0.049853   \n",
       "4   0.049922  0.038729  0.035209  0.045544  0.022806  0.042224  0.047781   \n",
       "5   0.017472  0.052891  0.019276  0.035988  0.031300  0.087265  0.050231   \n",
       "6   0.034805  0.033478  0.031627  0.047668  0.021135  0.015969  0.008824   \n",
       "7   0.414018  0.073205  0.034337  0.043209  0.027733  0.031051  0.031740   \n",
       "8   0.066599  0.372438  0.008824  0.046731  0.030432  0.034224  0.035970   \n",
       "9   0.033470  0.008824  0.400254  0.027105  0.040179  0.075849  0.039186   \n",
       "10  0.042618  0.050340  0.027423  0.407055  0.028338  0.020149  0.020659   \n",
       "11  0.028887  0.034373  0.043263  0.029891  0.438753  0.060326  0.051945   \n",
       "12  0.028015  0.033262  0.068729  0.018773  0.050733  0.358673  0.029300   \n",
       "13  0.033675  0.041629  0.042907  0.021882  0.052895  0.034541  0.448224   \n",
       "14  0.046453  0.018719  0.025165  0.017850  0.026394  0.025028  0.021403   \n",
       "15  0.026324  0.024315  0.061903  0.028065  0.031763  0.031520  0.025052   \n",
       "16  0.023483  0.030280  0.030907  0.038431  0.057121  0.029432  0.017420   \n",
       "\n",
       "          14        15        16  \n",
       "0   0.024582  0.043043  0.027549  \n",
       "1   0.044045  0.035595  0.021350  \n",
       "2   0.027746  0.027611  0.047359  \n",
       "3   0.071503  0.035478  0.019700  \n",
       "4   0.038591  0.013443  0.037133  \n",
       "5   0.013108  0.048441  0.018079  \n",
       "6   0.056614  0.093893  0.061204  \n",
       "7   0.045603  0.028351  0.023100  \n",
       "8   0.017503  0.024335  0.027575  \n",
       "9   0.024254  0.066038  0.029600  \n",
       "10  0.017495  0.029924  0.037162  \n",
       "11  0.027045  0.035982  0.058730  \n",
       "12  0.022499  0.030689  0.026152  \n",
       "13  0.022157  0.028460  0.017902  \n",
       "14  0.423379  0.050779  0.025843  \n",
       "15  0.045576  0.371969  0.046921  \n",
       "16  0.025905  0.052472  0.424885  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_text_rank_matrix(similarity_matrix, \n",
    "                         uniform_matrix,\n",
    "                         alpha = 0.15):\n",
    "    return alpha * uniform_matrix + (1 - alpha) * similarity_matrix\n",
    "\n",
    "text_rank_matrix = get_text_rank_matrix(normalized_cosine_similarity_matrix, uniform_matrix)\n",
    "\n",
    "pd.DataFrame(text_rank_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check The Matrix using Perron Frobenius Theorem**\n",
    "\n",
    "Perron Frobenius Theorem satifies these conditions:\n",
    "* All values in markov matrix is positive\n",
    "* Each rows in markov matrix sums to 1, and all values are non negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "289\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "print((text_rank_matrix >= 0).sum())\n",
    "print(text_rank_matrix.sum(axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the eigenfactors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.4918176  0.20696347 0.22797545 0.46795011 0.44642103\n",
      " 0.43191473 0.26524854 0.27269963 0.28872484 0.31396721 0.32402682\n",
      " 0.3541479  0.36500825 0.37066593 0.3925564  0.38664392]\n",
      "[-0.24200551 -0.26819835 -0.26980595 -0.24821263 -0.25392628 -0.22607829\n",
      " -0.27095827 -0.22868495 -0.24795614 -0.23490663 -0.23100659 -0.22023878\n",
      " -0.25680024 -0.2170586  -0.22420059 -0.24853495 -0.22397371]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(suppress = True)\n",
    "\n",
    "# Find the limiting/ Stationery Distribution\n",
    "# As mentioned by convention, we think of eigenvalue equation in term of column vectors instead of row vectors.\n",
    "# V is a column vector and P is a row vector\n",
    "\n",
    "# If np.eig(A) = Av = λv \n",
    "# If np.eig(A.T) = pA = λp or A^T p^T = λp^T\n",
    "\n",
    "# State distribution becomes column vector and our matrix is transposed.\n",
    "eigenvalues, normalized_eigenvectors = np.linalg.eig(text_rank_matrix.T)\n",
    "print(eigenvalues)\n",
    "print(normalized_eigenvectors[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.24200551, -0.26819835, -0.26980595, -0.24821263, -0.25392628,\n",
       "       -0.22607829, -0.27095827, -0.22868495, -0.24795614, -0.23490663,\n",
       "       -0.23100659, -0.22023878, -0.25680024, -0.2170586 , -0.22420059,\n",
       "       -0.24853495, -0.22397371])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether our eigenvector has the same value when multiplied by the matrix.\n",
    "# If it does, it's the proper vector.\n",
    "normalized_eigenvectors[:, 0].dot(text_rank_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the index with eigenvalue 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000000000007\n",
      "0.49181760001405883\n",
      "0.20696347318976407\n",
      "0.22797544972808556\n",
      "0.46795011069862275\n",
      "0.44642103400426\n",
      "0.4319147318754193\n",
      "0.2652485356459734\n",
      "0.27269963063794933\n",
      "0.2887248443033925\n",
      "0.31396721364928176\n",
      "0.32402682301694413\n",
      "0.35414790485210823\n",
      "0.365008254982586\n",
      "0.37066593281973365\n",
      "0.39255639703649414\n",
      "0.38664391526821784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_index_from_specified_value(my_array, find_value):\n",
    "    equivalent_indexes = []\n",
    "    for index, value in enumerate(my_array):\n",
    "        print(value)\n",
    "        if np.isclose(value, find_value):\n",
    "            equivalent_indexes.append(index)\n",
    "\n",
    "    return equivalent_indexes\n",
    "\n",
    "valid_indexes = get_index_from_specified_value(eigenvalues, 1.)\n",
    "valid_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.24200551]\n",
      " [-0.26819835]\n",
      " [-0.26980595]\n",
      " [-0.24821263]\n",
      " [-0.25392628]\n",
      " [-0.22607829]\n",
      " [-0.27095827]\n",
      " [-0.22868495]\n",
      " [-0.24795614]\n",
      " [-0.23490663]\n",
      " [-0.23100659]\n",
      " [-0.22023878]\n",
      " [-0.25680024]\n",
      " [-0.2170586 ]\n",
      " [-0.22420059]\n",
      " [-0.24853495]\n",
      " [-0.22397371]]\n"
     ]
    }
   ],
   "source": [
    "scores = normalized_eigenvectors[:, valid_indexes]\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if still holds probability distribution.  Eigenvectors are not unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05884566, 0.06521467, 0.06560557, 0.06035497, 0.06174429,\n",
       "       0.05497282, 0.06588577, 0.05560665, 0.06029261, 0.05711951,\n",
       "       0.05617118, 0.0535529 , 0.06244312, 0.05277961, 0.05451624,\n",
       "       0.06043335, 0.05446108])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = normalized_eigenvectors[:, 0] / normalized_eigenvectors[:, 0].sum()\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct method on finding limiting distribution. This is finding limiting distribution by brute force. We're going to the transitions until distribution converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "limiting_dist = np.ones(len(text_rank_matrix)) / len(text_rank_matrix)\n",
    "threshold = 1e-8 # To quit the llop\n",
    "delta = float('inf') # Show how much distribution has changed from one step to next.\n",
    "\n",
    "iters = 0\n",
    "while delta > threshold:\n",
    "    iters += 1\n",
    "\n",
    "    # Compute the distribution for the next step. (markov transition)\n",
    "    # By multiplying the distribution by X.\n",
    "    p = limiting_dist.dot(text_rank_matrix)\n",
    "\n",
    "    # Compute next delta. (compute change in limiting distribution)\n",
    "    # This is the sum of absolute differences between new and old distributions.\n",
    "    delta = np.abs(p - limiting_dist).sum()\n",
    "\n",
    "    # Update limiting distribution.\n",
    "    limiting_dist = p\n",
    "\n",
    "    print(iters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05884566, 0.06521467, 0.06560557, 0.06035497, 0.06174429,\n",
       "       0.05497282, 0.06588577, 0.05560665, 0.06029261, 0.05711951,\n",
       "       0.05617118, 0.0535529 , 0.06244312, 0.05277961, 0.05451624,\n",
       "       0.06043335, 0.05446108])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limiting_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if limiting_dist sums up to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999996"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "limiting_dist.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute sum of absolute differences before, and the answer we got by brute force."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.abs(normalized_eigenvectors[:, 0] / normalized_eigenvectors[:, 0] - limiting_dist).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Sentences if the value inside the eigenvector is more than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said.',\n",
       " 'The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%.',\n",
       " 'A number of retailers have already reported poor figures for December.',\n",
       " 'Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.',\n",
       " 'The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures.',\n",
       " 'The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s.',\n",
       " 'And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years.',\n",
       " '\"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don\\'t really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_summarized_text(sentences, scores, threshold = 0.06):\n",
    "    summarized_texts = []\n",
    "    for sentence, score in zip(sentences, scores):\n",
    "        if score > threshold:\n",
    "            summarized_texts.append(sentence)\n",
    "\n",
    "    return summarized_texts\n",
    "\n",
    "get_summarized_text(sentence_tokenized, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the answer with actual news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Christmas sales worst since 1981\n",
      "\n",
      "UK retail sales fell in December,\n",
      "failing to meet expectations and making it by some counts the worst\n",
      "Christmas since 1981.\n",
      "\n",
      "Retail sales dropped by 1% on the month in\n",
      "December, after a 0.6% rise in November, the Office for National\n",
      "Statistics (ONS) said.  The ONS revised the annual 2004 rate of growth\n",
      "down from the 5.9% estimated in November to 3.2%. A number of\n",
      "retailers have already reported poor figures for December.  Clothing\n",
      "retailers and non-specialist stores were the worst hit with only\n",
      "internet retailers showing any significant growth, according to the\n",
      "ONS.\n",
      "\n",
      "The last time retailers endured a tougher Christmas was 23 years\n",
      "previously, when sales plunged 1.7%.\n",
      "\n",
      "The ONS echoed an earlier\n",
      "caution from Bank of England governor Mervyn King not to read too much\n",
      "into the poor December figures.  Some analysts put a positive gloss on\n",
      "the figures, pointing out that the non-seasonally-adjusted figures\n",
      "showed a performance comparable with 2003. The November-December jump\n",
      "last year was roughly comparable with recent averages, although some\n",
      "way below the serious booms seen in the 1990s.  And figures for retail\n",
      "volume outperformed measures of actual spending, an indication that\n",
      "consumers are looking for bargains, and retailers are cutting their\n",
      "prices.\n",
      "\n",
      "However, reports from some High Street retailers highlight\n",
      "the weakness of the sector.  Morrisons, Woolworths, House of Fraser,\n",
      "Marks & Spencer and Big Food all said that the festive period was\n",
      "disappointing.\n",
      "\n",
      "And a British Retail Consortium survey found that\n",
      "Christmas 2004 was the worst for 10 years.  Yet, other retailers -\n",
      "including HMV, Monsoon, Jessops, Body Shop and Tesco - reported that\n",
      "festive sales were well up on last year.  Investec chief economist\n",
      "Philip Shaw said he did not expect the poor retail figures to have any\n",
      "immediate effect on interest rates.  \"The retail sales figures are\n",
      "very weak, but as Bank of England governor Mervyn King indicated last\n",
      "night, you don't really get an accurate impression of Christmas\n",
      "trading until about Easter,\" said Mr Shaw.  \"Our view is the Bank of\n",
      "England will keep its powder dry and wait to see the big picture.\"\n"
     ]
    }
   ],
   "source": [
    "print(textwrap.fill(sample_news.iloc[0], replace_whitespace=False, fix_sentence_endings=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Retail sales dropped by 1% on the month in December, after a 0.6% rise in November, the Office for National Statistics (ONS) said.',\n",
       " 'The ONS revised the annual 2004 rate of growth down from the 5.9% estimated in November to 3.2%.',\n",
       " 'A number of retailers have already reported poor figures for December.',\n",
       " 'Clothing retailers and non-specialist stores were the worst hit with only internet retailers showing any significant growth, according to the ONS.',\n",
       " 'The ONS echoed an earlier caution from Bank of England governor Mervyn King not to read too much into the poor December figures.',\n",
       " 'The November-December jump last year was roughly comparable with recent averages, although some way below the serious booms seen in the 1990s.',\n",
       " 'And a British Retail Consortium survey found that Christmas 2004 was the worst for 10 years.',\n",
       " '\"The retail sales figures are very weak, but as Bank of England governor Mervyn King indicated last night, you don\\'t really get an accurate impression of Christmas trading until about Easter,\" said Mr Shaw.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PageRank():\n",
    "    def _get_page_rank_matrix(self, sentences_tokenized, alpha = 0.15):\n",
    "        tf_idf_vectorizer = TfidfVectorizer(norm = 'l1')\n",
    "        tf_idf_matrix = tf_idf_vectorizer.fit_transform(sentence_tokenized)\n",
    "\n",
    "        cosine_similarity_matrix = cosine_similarity(\n",
    "            tf_idf_matrix\n",
    "        )\n",
    "\n",
    "        sum_all_rows = cosine_similarity_matrix.sum(axis = 1, dtype = 'float64')\n",
    "        normalized_cosine_similarity_matrix = cosine_similarity_matrix / sum_all_rows.reshape(-1, 1)\n",
    "        number_of_sentences = normalized_cosine_similarity_matrix.shape[0]\n",
    "        uniform_matrix = np.full(fill_value = 1 / number_of_sentences, shape = (number_of_sentences, number_of_sentences))\n",
    "        text_rank_matrix = alpha * uniform_matrix + (1 - alpha) * normalized_cosine_similarity_matrix\n",
    "        eigenvalues, normalized_eigenvectors = np.linalg.eig(text_rank_matrix.T)\n",
    "\n",
    "        return text_rank_matrix\n",
    "\n",
    "    def _get_scores(self, text_rank_matrix):\n",
    "        eigenvalues, normalized_eigenvectors = np.linalg.eig(text_rank_matrix.T)\n",
    "        valid_index = self._get_index_from_specified_value(eigenvalues, 1.)[0]\n",
    "        scores = normalized_eigenvectors[:, valid_index]\n",
    "        scores = normalized_eigenvectors[:, valid_index] / normalized_eigenvectors[:, valid_index].sum()\n",
    "        return scores\n",
    "\n",
    "    def _get_index_from_specified_value(self, my_array, find_value):\n",
    "        equivalent_indexes = []\n",
    "        for index, value in enumerate(my_array):\n",
    "            if np.isclose(value, find_value):\n",
    "                equivalent_indexes.append(index)\n",
    "\n",
    "        return equivalent_indexes\n",
    "\n",
    "    def _get_summarized_text(self, sentences, scores, threshold = 0.06):\n",
    "        summarized_texts = []\n",
    "        for sentence, score in zip(sentences, scores):\n",
    "            if score > threshold:\n",
    "                summarized_texts.append(sentence)\n",
    "\n",
    "        return summarized_texts\n",
    "\n",
    "    def summarize_text(self, sentences):\n",
    "        sentence_tokenized = sent_tokenize(sentences)\n",
    "        text_rank_matrix = self._get_page_rank_matrix(sentence_tokenized)\n",
    "        scores = self._get_scores(text_rank_matrix)\n",
    "        return self._get_summarized_text(sentence_tokenized, scores)\n",
    "        \n",
    "page_rank = PageRank()\n",
    "# print(stripped_sample_news)\n",
    "page_rank.summarize_text(stripped_sample_news.iloc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82ed002fa2d4956f5c6aec99bcefe0f73a9f79882f3c9e2319b14958a5896ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
